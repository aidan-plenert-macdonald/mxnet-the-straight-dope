{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes by Backprop with ``gluon`` (NN, classification)\n",
    "\n",
    "After discussing [Bayes by Backprop from scratch](https://github.com/zackchase/mxnet-the-straight-dope/blob/master/chapter18_variational-methods-and-uncertainty/bayes-by-backprop.ipynb) in a previous notebook, we can now look at the corresponding implementation as ``gluon`` components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start off with the usual set of imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import collections\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxnet import nd, autograd\n",
    "from matplotlib import pyplot as plt\n",
    "from mxnet import gluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For easy tuning and experimentation, we define a dictionary holding the hyper-parameters of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"num_hidden_layers\": 2,\n",
    "    \"num_hidden_units\": 400, \n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 10,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"num_samples\": 1,\n",
    "    \"pi\": 0.25,\n",
    "    \"sigma_p\": 1.0,\n",
    "    \"sigma_p1\": 0.75,\n",
    "    \"sigma_p2\": 0.01,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we specify the device context for MXNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST data\n",
    "\n",
    "We will again train and evaluate the algorithm on the MNIST data set and therefore load the data set as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def transform(data, label):\n",
    "    return data.astype(np.float32)/126.0, label.astype(np.float32)\n",
    "\n",
    "mnist = mx.test_utils.get_mnist()\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "batch_size = config['batch_size']\n",
    "\n",
    "train_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=True, transform=transform),\n",
    "                                      batch_size, shuffle=True)\n",
    "test_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=False, transform=transform),\n",
    "                                     batch_size, shuffle=False)\n",
    "\n",
    "num_train = sum([batch_size for i in train_data])\n",
    "num_batches = num_train / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reproduce and compare the results from the paper, we preprocess the pixels by dividing by 126."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition\n",
    "\n",
    "We can construct a few new Bayes gluon components that will facilitate our model training,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesDense(gluon.nn.Dense):\n",
    "    def __init__(self, *args, weight_std_initializer=None, bias_std_initializer='zeros', **kwargs):\n",
    "        super(BayesDense, self).__init__(*args, **kwargs)\n",
    "        with self.name_scope():\n",
    "            self.weight_std = self.params.get('weight_std', shape=(self._units, self._in_units),\n",
    "                                              init=weight_std_initializer,\n",
    "                                              allow_deferred_init=True)\n",
    "            if self.bias is not None:\n",
    "                self.bias_std = self.params.get('bias_std', shape=(self._units,),\n",
    "                                                init=bias_std_initializer,\n",
    "                                                allow_deferred_init=True)\n",
    "            else:\n",
    "                self.bias_std = None\n",
    "                \n",
    "    def _soft_add(self, F, x):\n",
    "        return F.sqrt(x**2 + 1e-5)\n",
    "        #return F.log(1.0 + F.exp(x))\n",
    "    \n",
    "    def get_shapes(self, weight_std, bias_std):\n",
    "        if not hasattr(weight_std, 'shape'):\n",
    "            r_shape = (self._units, self._in_units)\n",
    "        else:\n",
    "            r_shape = self.weight_std.shape\n",
    "        return r_shape, (self._units,)\n",
    "    \n",
    "    def bayes_loss(self, F, weight_std, bias_std=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes the extra terms in the Bayes loss function\n",
    "        \"\"\"\n",
    "        rw_shape, rb_shape = self.get_shapes(weight_std, bias_std)\n",
    "        \n",
    "        loss = nd.sum(\n",
    "            F.log(self._soft_add(F, weight_std)) + \n",
    "            F.random_normal(shape=rw_shape)**2/(2.0 * self._soft_add(F, weight_std)**2)\n",
    "        )\n",
    "        if bias_std is not None:\n",
    "            loss = loss + nd.sum(\n",
    "                F.log(self._soft_add(F, bias_std)) + \n",
    "                F.random_normal(shape=rb_shape)/(2.0 * self._soft_add(F, bias_std)**2)\n",
    "            )\n",
    "        return loss\n",
    "    \n",
    "    def hybrid_forward(self, F, x, weight, weight_std, bias=None, bias_std=None):\n",
    "        rw_shape, rb_shape = self.get_shapes(self.weight_std, self.bias_std)\n",
    "        \n",
    "        w = weight + F.random_normal(shape=rw_shape) * self._soft_add(F, weight_std)\n",
    "        \n",
    "        if bias is not None:\n",
    "            b = bias + F.random_normal(shape=rb_shape) * self._soft_add(F, bias_std)\n",
    "        else:\n",
    "            b = None\n",
    "        \n",
    "        return super(BayesDense, self).hybrid_forward(F, x, weight=w, bias=b)\n",
    "\n",
    "class BayesSequential(gluon.nn.HybridSequential):\n",
    "    def bayes_loss(self, F, **kwargs):\n",
    "        \"\"\"\n",
    "        Compute the extra terms in the Bayes loss function\n",
    "          here, we push the task down to each child assuming they\n",
    "          have a function `bayes_loss`\n",
    "        \"\"\"\n",
    "        def params(child):\n",
    "            if F is mx.ndarray:\n",
    "                # Normally, you'd call j.data(ctx), but context *shouldn't* matter\n",
    "                # here because all the parameters should be up-to-date, but don't quote me\n",
    "                return {i: j.data() for i, j in child._reg_params.items()}\n",
    "            elif F is mx.symbol:\n",
    "                return {i: j.var() for i, j in child._reg_params.items()}\n",
    "            else:\n",
    "                return {}\n",
    "        \n",
    "        return sum(\n",
    "            c.bayes_loss(F, **params(c), **kwargs) \n",
    "            if type(c) == BayesDense else 0.0\n",
    "            for c in self._children\n",
    "        )\n",
    "    \n",
    "class BayesSoftmaxCrossEntropyLoss(gluon.loss.SoftmaxCrossEntropyLoss):\n",
    "    def __init__(self, bayes_net, *args, prior_scale=1, **kwargs):\n",
    "        assert hasattr(bayes_net, 'bayes_loss')\n",
    "        super(BayesSoftmaxCrossEntropyLoss, self).__init__(*args, **kwargs)\n",
    "        self.prior_scale = prior_scale\n",
    "        with self.name_scope():\n",
    "            self.bayes_net = bayes_net\n",
    "    \n",
    "    def hybrid_forward(self, F, *args, **kwargs):\n",
    "        return (\n",
    "            super(BayesSoftmaxCrossEntropyLoss, self).hybrid_forward(F, *args, **kwargs) + \n",
    "            self.prior_scale * self.bayes_net.bayes_loss(F)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural net modeling\n",
    "\n",
    "As our model we are using a straightforward MLP and we are wiring up our network just as we are used to in ``gluon``. Note that we are not using any special layers during the definition of our network, as we believe that Bayes by Backprop should be thought of as a training method, rather than a speical architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = config['num_hidden_layers']\n",
    "num_hidden = config['num_hidden_units']\n",
    "\n",
    "net = BayesSequential()\n",
    "with net.name_scope():\n",
    "    for i in range(num_layers):\n",
    "        net.add(BayesDense(num_hidden, activation=\"relu\"))\n",
    "    net.add(BayesDense(num_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build objective/loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we define our loss function as described in [Bayes by Backprop from scratch](https://github.com/zackchase/mxnet-the-straight-dope/blob/master/chapter18_variational-methods-and-uncertainty/bayes-by-backprop.ipynb). Note that we are bundling all of this functionality as part of a ``gluon.loss.Loss`` subclass, where the loss computation is performed in the ``hybrid_forward`` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter initialization\n",
    "\n",
    "First, we need to initialize all the network's parameters, which are only point estimates of the weights at this point. We will soon see, how we can still train the netork in a Bayesian fashion, without interfering with the netowk's architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)\n",
    "softmax_cross_entropy = BayesSoftmaxCrossEntropyLoss(net, prior_scale=1.0/num_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have to forward-propagate a single data set entry once to set up all network parameters (weights and biases) with the desired initliaizer specified above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "Now, we still have to choose the optimizer we wish to use for training. This time, we are using the ``adam`` optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': config['learning_rate']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main training loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metric\n",
    "\n",
    "In order to being able to assess our model performance we define a helper function which evaluates our accuracy on an ongoing basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net, samples=10):\n",
    "    numerator = 0.\n",
    "    denominator = 0.\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(ctx)\n",
    "        \n",
    "        output = sum(net(data) for i in range(samples))/samples\n",
    "        \n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        \n",
    "        numerator += nd.sum(predictions == label)\n",
    "        denominator += data.shape[0]\n",
    "    return (numerator / denominator).asscalar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete loop\n",
    "\n",
    "The complete training loop is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 133037.859253, Train_acc 0.873017, Test_acc 0.8774\n"
     ]
    }
   ],
   "source": [
    "epochs = config['epochs']\n",
    "learning_rate = config['learning_rate']\n",
    "smoothing_constant = .01\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "for e in range(epochs):\n",
    "    for i, (data, label) in enumerate(train_data): \n",
    "        data = data.as_in_context(ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(ctx)\n",
    "        \n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "            loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        \n",
    "        # calculate moving loss for monitoring convergence\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (curr_loss if ((i == 0) and (e == 0)) \n",
    "                       else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
    "    \n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    train_acc.append(np.asscalar(train_accuracy))\n",
    "    test_acc.append(np.asscalar(test_accuracy))\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" %\n",
    "          (e, moving_loss, train_accuracy, test_accuracy))\n",
    "    \n",
    "plt.plot(train_acc)\n",
    "plt.plot(test_acc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration purposes, we can now take a look at one particular weight by plotting its distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def gaussian(x, mu, sigma):\n",
    "    scaling = 1.0 / nd.sqrt(2.0 * np.pi * (sigma ** 2))\n",
    "    bell = nd.exp(- (x - mu) ** 2 / (2.0 * sigma ** 2))\n",
    "\n",
    "    return scaling * bell\n",
    "\n",
    "def show_weight_dist(mean, variance):\n",
    "    sigma = nd.sqrt(variance)\n",
    "    x = np.linspace(mean.asscalar() - 4*sigma.asscalar(), mean.asscalar() + 4*sigma.asscalar(), 100)\n",
    "    plt.plot(x, gaussian(nd.array(x, ctx=ctx), mean, sigma).asnumpy())\n",
    "    plt.show()\n",
    "    \n",
    "mu = net._children[0].weight.data().asnumpy().flatten()[0]\n",
    "var = net._children[0].weight_std.data().asnumpy().flatten()[0]\n",
    "\n",
    "show_weight_dist(mu, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight pruning\n",
    "\n",
    "To measure the degree of redundancy present in the trained network and to reduce the model's parameter count, we now want to examine the effect of setting some of the weights to $0$ and evaluate the test accuracy afterwards. We can achieve this by ordering the weights according to their signal-to-noise-ratio, $\\frac{|\\mu_i|}{\\sigma_i}$, and setting a certain percentage of the weights with the lowest ratios to $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the signal-to-noise-ratio as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_to_noise_ratio(mus, sigmas):\n",
    "    sign_to_noise = []\n",
    "    for j in range(len(mus)):\n",
    "        sign_to_noise.extend([nd.abs(mus[j]) / sigmas[j]])\n",
    "    return sign_to_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We further introduce a few helper methods which turn our list of weights into a single vector containing all weights. This will make our subsequent actions easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_matrices_in_vector(vec):\n",
    "    for i in range(0, (num_layers + 1) * 2, 2):\n",
    "        if i == 0:\n",
    "            vec[i] = nd.reshape(vec[i], num_inputs * num_hidden)\n",
    "        elif i == num_layers * 2:\n",
    "            vec[i] = nd.reshape(vec[i], num_hidden * num_outputs)\n",
    "        else:\n",
    "            vec[i] = nd.reshape(vec[i], num_hidden * num_hidden)\n",
    "            \n",
    "    return vec\n",
    "\n",
    "def concact_vectors_in_vector(vec):\n",
    "    concat_vec = vec[0]\n",
    "    for i in range(1, len(vec)):\n",
    "        concat_vec = nd.concat(concat_vec, vec[i], dim=0)\n",
    "    \n",
    "    return concat_vec\n",
    "\n",
    "def transform_vector_structure(vec):\n",
    "    vec = vectorize_matrices_in_vector(vec)\n",
    "    vec = concact_vectors_in_vector(vec)\n",
    "    \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we also have a helper method which transforms the pruned weight vector back to the original layered structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "def prod(iterable):\n",
    "    return reduce(operator.mul, iterable, 1)\n",
    "\n",
    "def restore_weight_structure(vec):\n",
    "    pruned_weights = []\n",
    "    \n",
    "    index = 0\n",
    "    \n",
    "    for shape in shapes:\n",
    "        incr = prod(shape)\n",
    "        pruned_weights.extend([nd.reshape(vec[index : index + incr], shape)])\n",
    "        index += incr\n",
    "    \n",
    "    return pruned_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual pruning of the vector happens in the following function. Note that this function accepts an ordered list of percentages to evaluate the performance at different pruning rates. In this setting, pruning at each iteration means extracting the index of the lowest signal-to-noise-ratio weight and setting the weight at this index to $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def prune_weights(sign_to_noise_vec, prediction_vector, percentages):\n",
    "    pruning_indices = nd.argsort(sign_to_noise_vec, axis=0)\n",
    "    \n",
    "    for percentage in percentages:\n",
    "        prediction_vector = mus_copy_vec.copy()\n",
    "        pruning_indices_percent = pruning_indices[0:int(len(pruning_indices)*percentage)]\n",
    "        for pr_ind in pruning_indices_percent:\n",
    "            prediction_vector[int(pr_ind.asscalar())] = 0\n",
    "        pruned_weights = restore_weight_structure(prediction_vector)\n",
    "        test_accuracy = evaluate_accuracy(test_data, net, pruned_weights)\n",
    "        print(\"%s --> %s\" % (percentage, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting the above function together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_to_noise = signal_to_noise_ratio(raw_mus, sigmas)\n",
    "sign_to_noise_vec = transform_vector_structure(sign_to_noise)\n",
    "\n",
    "mus_copy = raw_mus.copy()\n",
    "mus_copy_vec = transform_vector_structure(mus_copy)\n",
    "\n",
    "prune_weights(sign_to_noise_vec, mus_copy_vec, [0.1, 0.25, 0.5, 0.75, 0.95, 0.98, 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the number of units used in the original network, the highest achievable pruning percentages (without significantly reducing the predictive performance) can vary. The paper, for example, reports almost no change in the test accuracy when pruning 95% of the weights in a 1200 unit Bayesian neural network, which creates a significantly sparser network, leading to faster predictions and reduced memory requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have taken a look at an efficient Bayesian treatment for neural networks using variational inference via the \"Bayes by Backprop\" algorithm (introduced by the \"[Weight Uncertainity in Neural Networks](https://arxiv.org/abs/1505.05424)\" paper). We have implemented a stochastic version of the variational lower bound and optimized it in order to find an approximation to the posterior distribution over the weights of a MLP network on the MNIST data set. As a result, we achieve regularization on the network's parameters and can quantify our uncertainty about the weights accurately. Finally, we saw that it is possible to significantly reduce the number of weights in the neural network after training while still keeping a high accuracy on the test set.\n",
    "\n",
    "We also note that, given this model implementation, we were able to reproduce the paper's results on the MNIST data set, achieving a comparable test accuracy for all documented instances of the MNIST classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For whinges or inquiries, [open an issue on  GitHub.](https://github.com/zackchase/mxnet-the-straight-dope)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
