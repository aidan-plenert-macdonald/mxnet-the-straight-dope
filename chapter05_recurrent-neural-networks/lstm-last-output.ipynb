{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using just the last output of LSTM per sequence\n",
    "\n",
    "Lets imagine you want to classify sequences. Sometimes it might not make sense to classify on each element of the sequence, so you need to use just the last output (or the last N outputs) of your LSTM.\n",
    "\n",
    "Lets generate some sequences that we might care about. You can choose the example you want by selecting the proper index to `EXAMPLE` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the hard example\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxnet import nd, autograd, sym\n",
    "from mxnet import gluon\n",
    "\n",
    "EXAMPLE = ['simple', 'long-term-test', 'hard'][2]\n",
    "print('Running the', EXAMPLE, 'example')\n",
    "\n",
    "if EXAMPLE == 'simple':\n",
    "    X, y = list(zip(*[(label*np.ones((np.random.randint(low=5, high=100),)), label)\n",
    "                      for label in np.random.choice(range(3), size=1000)]))\n",
    "elif EXAMPLE == 'long-term-test':\n",
    "    X, y = list(zip(*[(1.0*(np.arange(np.random.randint(low=5, high=100)) <= label), label)\n",
    "                      for label in np.random.choice(range(3), size=1000)]))\n",
    "elif EXAMPLE == 'hard':\n",
    "    X, y = list(zip(*[(1.0*(np.random.random((np.random.randint(low=5, high=100),)) > 1.0/(label+2.0)), label)\n",
    "                      for label in np.random.choice(range(3), size=1000)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot a couple of these sequences to understand what they look like,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNxGRZ9i4iYg8w8ZNROQZNm4iIs/8P17b/LugepSqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x102b45e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i, x in enumerate(X[:4]):\n",
    "    plt.plot(x + 2*i, 'o')\n",
    "\n",
    "plt.legend(y[:4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is that some sequences have more \"ups\" than \"downs\" and each sequence is a different length. We'd like to be able to classify each sequence into the percent of ups to downs.\n",
    "\n",
    "Lets define a new kind of network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LastNet(gluon.Block):\n",
    "    def __init__(self, recurrent_width, forward_width, **kwargs):\n",
    "        super(LastNet, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self._rnn = gluon.rnn.LSTM(recurrent_width)\n",
    "            self._nn  = gluon.nn.Dense(forward_width)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self._rnn(x)\n",
    "        return self._nn(h[h.shape[0]-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to check that the gradients will get computed properly. I was skeptical, so I wrote this quick test that constructs an arbitrary sequence, pushes it through an LSTM, and then computes the loss using only the last output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient using only last output for sequence length 1 \n",
      "\tlstm0_l0_i2h_weight:[-0.03023597  0.         -3.27786589 -0.028655  ] \n",
      "\tlstm0_l0_h2h_weight:[ 0.  0.  0.  0.] \n",
      "\tlstm0_l0_i2h_bias:[-0.01007866 -0.         -1.09262192 -0.00955167] \n",
      "\tlstm0_l0_h2h_bias:[-0.01007866 -0.         -1.09262192 -0.00955167] \n",
      "\n",
      "\n",
      "Gradient using only last output for sequence length 2 \n",
      "\tlstm1_l0_i2h_weight:[-0.31645533 -0.10482672 -3.71678162 -0.34464899] \n",
      "\tlstm1_l0_h2h_weight:[-0.0025947  -0.00127264 -0.03042663 -0.00417328] \n",
      "\tlstm1_l0_i2h_bias:[-0.10548511 -0.03494224 -1.23892713 -0.114883  ] \n",
      "\tlstm1_l0_h2h_bias:[-0.10548511 -0.03494224 -1.23892713 -0.114883  ] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for length in [1, 2]:\n",
    "    x = nd.array([3]*length).reshape((-1, 1, 1))\n",
    "\n",
    "    layer = gluon.rnn.LSTM(1)\n",
    "    layer.initialize()\n",
    "    with autograd.record():\n",
    "        out = layer(x)\n",
    "        loss = nd.mean((out[out.shape[0]-1] - 2)**2)\n",
    "    loss.backward()\n",
    "\n",
    "    print('Gradient using only last output for sequence length', length, \n",
    "          *['\\n\\t%s:%s' % (name, p.grad().asnumpy().flatten()) for name, p in layer.params.items()], '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the hidden to hidden weights (ie. `h2h_weight`) only have a gradient when the sequence is length 2 or more. So I feel comfortable assuming that the gradients are computed properly.\n",
    "\n",
    "Initialize the parameters,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXAMPLE == 'hard':\n",
    "    BATCH_SIZE=20\n",
    "    EPOCHS=60\n",
    "else:\n",
    "    BATCH_SIZE=20\n",
    "    EPOCHS=20\n",
    "\n",
    "ctx = mx.cpu()\n",
    "\n",
    "net = LastNet(100, 3)\n",
    "net.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': .1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And train,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch in np.random.choice(len(X), size=(int(len(X)/BATCH_SIZE), BATCH_SIZE)):\n",
    "        with autograd.record():\n",
    "            loss = sum(softmax_cross_entropy(net(nd.array(X[i]).reshape(shape=(-1, 1, 1))),\n",
    "                                             nd.array([y[i]]).reshape((-1, 1)))\n",
    "                       for i in batch)\n",
    "        loss.backward()\n",
    "        trainer.step(BATCH_SIZE)\n",
    "        losses.append(loss.asscalar())\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick demo that it actually learned something,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = [nd.softmax(net(nd.array(x).reshape((-1, 1, 1))), axis=1).asnumpy().flatten().argmax() for x in X]\n",
    "confusion_matrix(y, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
